# Retrieval King - Claude Code Context

**Last Updated**: November 1, 2024
**Project Status**: COMPLETE & PRODUCTION-READY

## Quick Reference

### Virtual Environment
```bash
# ALWAYS activate before working on backend
source retrieval_king/bin/activate

# Python version: 3.12 (installed via Homebrew at /opt/homebrew/bin/python3.12)
# Venv location: ./retrieval_king/
```

### Running the Application

**Local Development** (2 terminals needed):
```bash
# Terminal 1: Backend
source retrieval_king/bin/activate
cd backend && uvicorn main:app --reload

# Terminal 2: Frontend
cd frontend && npm run dev
```

**Docker**:
```bash
docker-compose up --build
# Frontend: http://localhost:3000
# Backend: http://localhost:8000/docs
```

### Git Workflow
1. **Make changes** to code files
2. **Test locally** to ensure functionality
3. **Stage changes**: `git add -A`
4. **Commit with message**:
   ```bash
   git commit -m "$(cat <<'EOF'
   Brief description of changes

   More detailed explanation if needed.
   - Bullet points for multiple changes

   ðŸ¤– Generated with Claude Code

   Co-Authored-By: Claude <noreply@anthropic.com>
   EOF
   )"
   ```
5. **Push to GitHub**: `git push origin main`

## Project Overview

### What is Retrieval King?
A full-stack RAG (Retrieval-Augmented Generation) application that processes documents via OCR and enables intelligent question-answering with source citations.

### Tech Stack
- **Backend**: FastAPI + Python 3.12
- **Frontend**: React + Vite + TypeScript + Tailwind CSS
- **Vector DB**: ChromaDB
- **Models**:
  - DeepSeek-OCR (3B, vision-language for document processing)
  - IBM Granite Embedding 30M (384-dim, 512-token context)
  - IBM Granite Reranker (149M, 8K-token context)
  - GPT-4o-mini (query rewriting + response generation)

## Architecture Overview

```
Frontend (React/Vite) â† HTTP â†’ FastAPI Backend
                                â†“
                         LangGraph Workflow
                         â”œâ”€ Query Classification
                         â”œâ”€ Query Rewriting (optional)
                         â”œâ”€ Retrieval (single/parallel)
                         â”œâ”€ Reranking
                         â””â”€ Response Generation
                                â†“
                    ChromaDB + OpenAI API
```

## Project Structure

```
Retrieval_King/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ core/config.py           # Settings (env vars, paths, defaults)
â”‚   â”‚   â”œâ”€â”€ models/schemas.py        # Pydantic models
â”‚   â”‚   â”œâ”€â”€ services/                # Core business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ ocr_service.py       # DeepSeek-OCR
â”‚   â”‚   â”‚   â”œâ”€â”€ embedding_service.py # Granite 30M embeddings
â”‚   â”‚   â”‚   â”œâ”€â”€ reranker_service.py  # Granite reranker
â”‚   â”‚   â”‚   â”œâ”€â”€ chunking_service.py  # LangChain chunking
â”‚   â”‚   â”‚   â”œâ”€â”€ vector_store.py      # ChromaDB wrapper
â”‚   â”‚   â”‚   â””â”€â”€ llm_service.py       # OpenAI wrapper
â”‚   â”‚   â”œâ”€â”€ graph/rag_graph.py       # LangGraph workflow definition
â”‚   â”‚   â””â”€â”€ api/
â”‚   â”‚       â”œâ”€â”€ upload.py            # Document upload endpoints
â”‚   â”‚       â””â”€â”€ query.py             # Query endpoints
â”‚   â”œâ”€â”€ main.py                      # FastAPI app entry point
â”‚   â””â”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ FileUpload.tsx       # Drag-and-drop upload
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatInterface.tsx    # Message history + query
â”‚   â”‚   â”‚   â”œâ”€â”€ CitationPopup.tsx    # Source attribution modal
â”‚   â”‚   â”‚   â””â”€â”€ DocumentManager.tsx  # Document list
â”‚   â”‚   â”œâ”€â”€ services/api.ts          # API client
â”‚   â”‚   â”œâ”€â”€ App.tsx                  # Main layout
â”‚   â”‚   â””â”€â”€ main.tsx                 # Entry point
â”‚   â”œâ”€â”€ vite.config.ts               # Build config
â”‚   â”œâ”€â”€ tailwind.config.js           # Styling
â”‚   â””â”€â”€ package.json                 # Dependencies
â”œâ”€â”€ docker-compose.yml               # Container orchestration
â”œâ”€â”€ README.md                        # User-facing documentation
â”œâ”€â”€ SETUP_GUIDE.md                   # Detailed setup instructions
â””â”€â”€ .env.example                     # Configuration template
```

## Key Technical Decisions

### Chunking (Critical!)
- **Max size**: 450 tokens (Granite 30M hard limit is 512)
- **Overlap**: 75 tokens
- **Strategy**: Recursive sentence-aware splitting via LangChain
- **Tokenizer**: Uses Granite tokenizer for accuracy

### Retrieval Pipeline
1. Initial retrieval: Top-100 via cosine similarity
2. Reranking: Cross-encoder scores top-100 â†’ top-10
3. Response: LLM generates answer with inline citations

### Query Rewriting
- LLM classifies if query is complex/broad
- If yes: Generate 2-3 variants + parallel retrieval + merge results
- If no: Direct single retrieval

### Citation System
- Inline citations `[1] [2]` [3]` in response
- Click to open modal with:
  - Full chunk text
  - Source filename
  - Page number (if available)
  - Confidence score
  - Chunk ID & Document ID

## Configuration (`.env` file)

Required:
```
OPENAI_API_KEY=sk-your-key-here
```

Key tuning parameters:
```
CHUNK_SIZE=450              # Max tokens (default: good for most cases)
CHUNK_OVERLAP=75            # Token overlap (increase for more context)
RETRIEVAL_TOP_K=100         # Initial candidates (increase for recall)
RERANK_TOP_K=10             # Final results (decrease for speed)
DEVICE=cuda                 # Use 'cpu' if no GPU
```

## Common Development Tasks

### Adding a New Dependency
```bash
source retrieval_king/bin/activate
cd backend
pip install package-name
pip freeze > requirements.txt
git add requirements.txt && git commit -m "Add package-name dependency"
```

### Running Tests
```bash
source retrieval_king/bin/activate
cd backend
pytest  # if tests are added
```

### Debugging Backend
- Add `print()` statements or use Python debugger
- Check logs in terminal where `uvicorn` runs
- API docs at `http://localhost:8000/docs`

### Debugging Frontend
- Browser console (F12 or Cmd+Option+I)
- React DevTools browser extension
- Network tab to inspect API calls

### Viewing Database
ChromaDB persists to `./data/chroma/`
```bash
# Clear all embeddings (keeps data dir structure)
rm -rf data/chroma/*

# Backup before clearing
cp -r data/chroma data/chroma.backup
```

## Important Notes for Future Instances

### Model Loading
- **First run**: Models download automatically (~15-20GB)
- **Subsequent runs**: Models cached in `./data/models/`
- **GPU required**: For OCR (DeepSeek-OCR is 3B params)
- **CPU fallback**: Available but slow

### File Upload Flow
1. User uploads file via React drag-and-drop
2. FastAPI saves to `./data/uploads/`
3. Background task processes:
   - OCR extraction
   - Chunking
   - Embedding
   - Vector store insertion
4. File deleted after processing (only embeddings kept)

### Important Constraints
- Granite 30M context window: **512 tokens** (hard limit)
- Chunk size must respect this: set to 450 (with buffer)
- Reranker can handle 8K tokens (allows longer documents temporarily)
- GPT-4o-mini used for both query rewriting AND response generation
  - Could be optimized by using different models
  - Framework supports this via `set_generator_model()` and `set_query_rewriter_model()`

### Testing Documents
Good test files:
- PDF documents (technical papers work well)
- Screenshots/images with text
- Office documents (DOCX, PPTX)
- Scanned/low-quality images (tests OCR robustness)

### Performance Baselines
- Document upload (10MB PDF): 5-30s
- Query response: 1.5-3.5s total
- Breakdown: Retrieval 100-300ms + Rerank 100-200ms + Generation 1-3s

## Commit Message Convention

Always follow this format:
```
Title of change (one line)

Detailed explanation of what changed and why.
- Use bullet points for multiple changes
- Be specific about which files/features

ðŸ¤– Generated with Claude Code

Co-Authored-By: Claude <noreply@anthropic.com>
```

Example good commits:
- "Implement Granite Reranker service with two-stage retrieval"
- "Add FileUpload component with drag-and-drop support"
- "Fix citation tracking in RAG workflow"
- "Update configuration with new environment variables"

## If You Get Stuck

1. **Backend won't start**: Check if venv is activated + all deps installed
2. **Frontend can't reach backend**: Check CORS in `main.py`, ensure backend is running
3. **GPU memory errors**: Set `DEVICE=cpu` or reduce batch sizes in services
4. **Models downloading too slow**: Check internet connection, consider pre-downloading
5. **Citation links not working**: Check `CitationPopup` component in React
6. **Chunks too large**: Lower `CHUNK_SIZE` in `.env`, reprocess documents

## Next Steps for Development

Completed features (production-ready):
- âœ… OCR document processing
- âœ… Intelligent chunking
- âœ… Semantic retrieval
- âœ… Query rewriting
- âœ… Reranking
- âœ… Citation tracking
- âœ… Full-stack UI

Potential enhancements:
- [ ] Conversation history
- [ ] Multi-language support
- [ ] PDF table extraction
- [ ] Web search integration
- [ ] Fine-tuned embeddings
- [ ] GraphRAG entity relationships
- [ ] Streaming responses UI improvements
- [ ] User authentication
- [ ] Rate limiting

## Quick Reminders

1. **Always activate venv** before working on backend
2. **Both terminals needed** for local dev (backend + frontend)
3. **Commit frequently** with clear messages
4. **Test locally** before pushing
5. **Check `.env`** is set up with OpenAI API key
6. **GPU optional** but strongly recommended (OCR is 3B params)
7. **Models cache** to `./data/models/` - large downloads, be patient
8. **ChromaDB persists** to `./data/chroma/` - backup before clearing

## Useful Commands Cheatsheet

```bash
# Activate venv
source retrieval_king/bin/activate

# Run backend
cd backend && uvicorn main:app --reload

# Run frontend
cd frontend && npm run dev

# Install new Python package
pip install package-name && pip freeze > requirements.txt

# Docker full stack
docker-compose up --build

# Check git status
git status

# View recent commits
git log --oneline | head -10

# Clear all local data
rm -rf data/

# Reset to last commit
git reset --hard HEAD
```

---

**Remember**: This project is production-ready. Focus on testing thoroughly before committing and pushing!
